{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from multiprocessing import Pool\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for scraping information from the website landing page\n",
    "landing_page_config = {\n",
    "    'url': {\n",
    "        'class': '_mm360j',\n",
    "        'attribute': 'href'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Config for scraping information from each listing page\n",
    "internal_page_config = {\n",
    "    'listing_name': {\n",
    "        'class': '_fecoyn4'\n",
    "    },\n",
    "    'listing_type': {\n",
    "        'class': '_tqmy57'\n",
    "    },\n",
    "    'star_rating': {\n",
    "        'class': '_1ne5r4rt'\n",
    "    },\n",
    "    'price': {\n",
    "        'class': '_1jo4hgw'\n",
    "    },\n",
    "    'review': {\n",
    "        'class': '_162hp8xh'\n",
    "    },\n",
    "    'amenities': {\n",
    "        'class': '_19xnuo97'\n",
    "    },\n",
    "    'num_reviews': {\n",
    "        'class': '_1qf7wt4w'\n",
    "    },\n",
    "    'location_name': {\n",
    "        'class': '_pbq7fmm'\n",
    "    },\n",
    "    'owner_info': {\n",
    "        'class': 'tehcqxo.dir.dir-ltr'\n",
    "    },\n",
    "    'owner_details': {\n",
    "        'class': '_88xxct'\n",
    "    },\n",
    "    'house_timings': {\n",
    "        'class': 'c1lue5su.dir.dir-ltr'\n",
    "    },\n",
    "    'listing_highlights': {\n",
    "        'class': '_1vjikx5'\n",
    "    },\n",
    "    'response_times': {\n",
    "        'class': 'fhhmddr.dir.dir-ltr'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Link config\n",
    "base_link = 'https://www.airbnb.com/s/'\n",
    "\n",
    "city_names = ['Palo-Alto--CA--United-States/']\n",
    "# , 'New-York--NY--United-States/', 'Washington--D.C.--USA/', 'College-Park--MD--United-States/', 'Palo-Alto--CA--United-States/', 'Dallas--TX--United-States/'\n",
    "\n",
    "extension = '?items_offset='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the website\n",
    "class AirbnbParser:\n",
    "    def __init__(self):\n",
    "        self.curr_city = None\n",
    "        self.final_data = None\n",
    "        pass\n",
    "\n",
    "    def get_driver(self):\n",
    "        \"\"\"\n",
    "        Returns a new instance of selenium webdriver\n",
    "        \"\"\"\n",
    "        return webdriver.Chrome('/usr/local/bin/chromedriver')\n",
    "\n",
    "    def get_next_link(self, curr_offset, city, offset=20):\n",
    "        \"\"\"\n",
    "        Generates next link from the current parsed link\n",
    "        \"\"\"\n",
    "        return ''.join([base_link, city, 'homes', extension]) + str(curr_offset + offset)\n",
    "\n",
    "    def get_url_list_from_landing_page(self, start_index, city):\n",
    "        \"\"\"\n",
    "        Scrapes the landing pages to get list of listing URLs\n",
    "        \"\"\"\n",
    "        driver = self.get_driver()\n",
    "        link = self.get_next_link(0, city, start_index)\n",
    "        driver.get(link)\n",
    "        for key, value in landing_page_config.items():\n",
    "            class_name = value['class']\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, class_name)))\n",
    "            except:\n",
    "                pass\n",
    "            url_list = driver.find_elements(By.CLASS_NAME, class_name)\n",
    "            attribute = value['attribute']\n",
    "            if key in self.final_data.keys():\n",
    "                self.final_data[key] += [data_val.get_attribute(attribute) for data_val in url_list]\n",
    "            else:\n",
    "                self.final_data[key] = [data_val.get_attribute(attribute) for data_val in url_list]\n",
    "            if not url_list:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def get_data_from_listing_page(self, link):\n",
    "        \"\"\"\n",
    "        Scrapes each internal page to get data of each listing\n",
    "        \"\"\"\n",
    "        driver = self.get_driver()\n",
    "        driver.get(link)\n",
    "        result = {}\n",
    "        for _, value in internal_page_config.items():\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, value['class'])))\n",
    "            except:\n",
    "                pass\n",
    "        for key, value in internal_page_config.items():\n",
    "            scraped_data = driver.find_elements(By.CLASS_NAME, value['class'])\n",
    "            result[key] = [data_val.text for data_val in scraped_data]\n",
    "        result['url'] = link\n",
    "        result['city'] = self.curr_city\n",
    "        return result\n",
    "    \n",
    "    def scrape_pages(self, curr_city):\n",
    "        index = 0\n",
    "        continue_scraping = self.get_url_list_from_landing_page(index, curr_city)\n",
    "        index+=20\n",
    "        while continue_scraping and index<281:\n",
    "            continue_scraping = self.get_url_list_from_landing_page(index, curr_city)\n",
    "            index+=20\n",
    "        with Pool(5) as pool:\n",
    "            result = pool.map(self.get_data_from_listing_page, self.final_data['url'])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return result\n",
    "            \n",
    "    def start_scraping(self):\n",
    "        final_data = []\n",
    "        for city in city_names:\n",
    "            self.final_data = {}\n",
    "            self.curr_city = city\n",
    "            final_data += self.scrape_pages(city)\n",
    "        return final_data\n",
    "            \n",
    "scraper = AirbnbParser()\n",
    "final_data = scraper.start_scraping()\n",
    "listings_df = pd.DataFrame(final_data)\n",
    "listings_df.to_csv('California.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ny data intp df\n",
    "df_listings = pd.read_csv('NewYork.csv', sep = ',', error_bad_lines=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Id column\n",
    "df_listings['listing_id'] = df_listings['url'].str.extract('\\/rooms\\/(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-arrange columns \n",
    "column_names = list(df_listings.columns.values)\n",
    "column_names.insert(0, column_names.pop())\n",
    "df_listings = df_listings.reindex(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df for amenities\n",
    "df_amenities = df_listings[['listing_id', 'amenities']]\n",
    "df_amenities.set_index('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process amenities further\n",
    "\n",
    "# df_amenities.unstack()['amenities'].str.strip('[]').str.split(', ', expand=True).value_counts()\n",
    "# res = df_amenities.set_index(['listing_id'])['amenities'].apply(pd.Series).stack()\n",
    "# res = res.reset_index()\n",
    "# res.columns = ['listing_id','level_1','amenities']\n",
    "# res.drop(columns='level_1', inplace = True)\n",
    "# res\n",
    "# df_amenities['amenities'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean price data\n",
    "df_listings['price'] = df_listings['price'].str.extract('\\$(\\d*\\.?\\d*)').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check records having bad data for price\n",
    "\n",
    "# def is_integer_num(n):\n",
    "#     if isinstance(n, int):\n",
    "#         return True\n",
    "#     if isinstance(n, float):\n",
    "#         return not n.is_integer()\n",
    "#     return False\n",
    "# df_listings[df_listings['price'].apply(is_integer_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean star_rating \n",
    "df_listings['star_rating'] = df_listings['star_rating'].str.extract('(\\d*\\.\\d*)').astype(float)\n",
    "# fill na with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of reviews\n",
    "\n",
    "df_listings['num_reviews'] = df_listings['num_reviews'].str.extract('(\\d+)\\s+review').astype(float)\n",
    "# len(df_listings[df_listings['num_reviews'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicode(listing_name):\n",
    "    text_encode = listing_name.encode(encoding=\"ascii\", errors=\"ignore\")\n",
    "    text_decode = text_encode.decode()\n",
    "    return \" \".join([word for word in text_decode.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation) \n",
    "\n",
    "def clean_tags(listing_name):\n",
    "    # removing mentions e.g @CityBoy12\n",
    "    listing_name = re.sub(\"@\\S+\", \"\", listing_name)\n",
    "    # remove $ signs\n",
    "    listing_name = re.sub(\"\\$\", \"\", listing_name)\n",
    "    # remove urls\n",
    "    listing_name = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", listing_name)\n",
    "    # remove hashtags\n",
    "    listing_name = re.sub(\"#\", \"\", listing_name)\n",
    "    # remove punctations\n",
    "    listing_name = \"\".join([ch for ch in listing_name if ch not in punct])\n",
    "    return listing_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean listing names\n",
    "df_listings['listing_name'] = df_listings['listing_name'].apply(clean_unicode)\n",
    "df_listings['listing_name'] = df_listings['listing_name'].apply(clean_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean listing type\n",
    "\n",
    "df_listing_type = df_listings[['listing_id', 'listing_type']]\n",
    "listing_type_pattern = \"\"\"\\['(?P<type>[\\w*\\s*]*)hosted by\\s*(?P<host>[\\w*\\s*]*)\\\\\\\\n(?P<no_of_guests>\\d*)\\s*guest[s]?\\s*.\\s*(?P<no_of_bedrooms>\\d*)\\s*bedroom\\s*.\\s*(?P<no_of_beds>\\d*)\\s*bed[s]?\\s*.\\s*(?P<no_of_bath>\\d*)\"\"\"\n",
    "df_listing_type = df_listing_type['listing_type'].str.extract(listing_type_pattern, expand=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_type.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
